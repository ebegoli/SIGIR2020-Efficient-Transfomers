{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sentence Transformers](https://www.sbert.net/index.html) is a Python framework for sentence, text, and image embeddings based on the Sentence-BERT [paper](https://arxiv.org/abs/1908.10084)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB4PkWm8-pfn"
   },
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:30:25.940358Z",
     "start_time": "2022-07-07T00:30:25.835748Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, pickle, sys, warnings, tqdm, time, torch, json, gzip\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "\n",
    "import os\n",
    "import bokeh, bokeh.models, bokeh.plotting\n",
    "import tensorflow.compat.v2 as tf\n",
    "from simpleneighbors import SimpleNeighbors\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load a pre-trained model. For this tutorial we will use the `all-mpnet-base-v2` model which was trained on all available training data (more than 1 billion training pairs) and is designed as a general purpose model. `sentence_transformers` have other pretrained models that can be found [here](https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:30:27.141448Z",
     "start_time": "2022-07-07T00:30:26.009736Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:30:26.007845Z",
     "start_time": "2022-07-07T00:30:25.943558Z"
    }
   },
   "outputs": [],
   "source": [
    "def bm25_tokenizer(text):\n",
    "  tokenized_doc = []\n",
    "  for token in text.lower().split():\n",
    "    token = token.strip(string.punctuation)\n",
    "\n",
    "    if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "      tokenized_doc.append(token)\n",
    "  return tokenized_doc\n",
    "\n",
    "def search(query):\n",
    "  print(\"Input question:\", query)\n",
    "\n",
    "  ##### BM25 search (lexical search) #####\n",
    "  bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "  top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "  bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "  bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "  print(\"Top-3 lexical search (BM25) hits\")\n",
    "  for hit in bm25_hits[0:3]:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "  ##### Sematic Search #####\n",
    "  # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "  question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "  question_embedding = question_embedding.cuda()\n",
    "  hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "  hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "  ##### Re-Ranking #####\n",
    "  # Now, score all retrieved passages with the cross_encoder\n",
    "  cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "  cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "  # Sort results by the cross-encoder scores\n",
    "  for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "  # Output of top-5 hits from bi-encoder\n",
    "  print(\"\\n-------------------------\\n\")\n",
    "  print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "  hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "  for hit in hits[0:3]:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "  # Output of top-5 hits from re-ranker\n",
    "  print(\"\\n-------------------------\\n\")\n",
    "  print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "  hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "  for hit in hits[0:3]:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Textual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence embeddings can be used to compare how semantically similar two sentences are. A common way to compare semantic content is to use `cosine` similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:30:27.334996Z",
     "start_time": "2022-07-07T00:30:27.143992Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'The cat sits outside',\n",
    "  'A man is playing guitar',\n",
    "  'I love pasta',\n",
    "  'The new movie is awesome',\n",
    "  'The cat plays in the garden',\n",
    "  'A woman watches TV',\n",
    "  'The new movie is so great',\n",
    "  'Do you like pizza?',\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "pairs = []\n",
    "for i in range(len(cosine_scores)-1):\n",
    "  for j in range(i+1, len(cosine_scores)):\n",
    "    pairs.append({'index': [i, j], 'score': cosine_scores[i][j].cpu().numpy()})\n",
    "\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "sim = {'Sentence 1': [], 'Sentence 2': [], 'Score': []}\n",
    "for pair in pairs[0:10]:\n",
    "  i, j = pair['index']\n",
    "  sim['Sentence 1'].append(sentences[i])\n",
    "  sim['Sentence 2'].append(sentences[j])\n",
    "  sim['Score'].append(pair['score'])\n",
    "\n",
    "pd.DataFrame(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic search improves search accuracy by understanding the content of the search query. In addition to finding documents based on lexical matches, semantic search can also synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:30:27.476209Z",
     "start_time": "2022-07-07T00:30:27.337725Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "  'A man is eating food.',\n",
    "  'A man is eating a piece of bread.',\n",
    "  'The girl is carrying a baby.',\n",
    "  'A man is riding a horse.',\n",
    "  'A woman is playing violin.',\n",
    "  'Two men pushed carts through the woods.',\n",
    "  'A man is riding a white horse on an enclosed ground.',\n",
    "  'A monkey is playing drums.',\n",
    "  'A cheetah is running behind its prey.'\n",
    "]\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = [\n",
    "  'A man is eating pasta.',\n",
    "  'Someone in a gorilla costume is playing a set of drums.',\n",
    "  'A cheetah chases prey on across a field.'\n",
    "]\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "  query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "  # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "  cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "  top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "  print(\"\\n\\n======================\\n\\n\")\n",
    "  print(\"Query:\", query)\n",
    "#   print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "  \n",
    "  sim = {'Similar Sentence': [], 'Similarity Score': []}  \n",
    "  for score, idx in zip(top_results[0], top_results[1]):\n",
    "    sim['Similar Sentence'].append(corpus[idx])\n",
    "    sim['Similarity Score'].append(score.cpu().numpy())\n",
    "  \n",
    "  display(pd.DataFrame(sim))\n",
    "\n",
    "#         print(corpus[idx], \"(Score: {:.4f})\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrase Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paraphrase mining is the task of finding texts with identical or similar meaning in a large corpus of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:30:27.538779Z",
     "start_time": "2022-07-07T00:30:27.477972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Single list of sentences - Possible tens of thousands of sentences\n",
    "sentences = [\n",
    "  'The cat sits outside',\n",
    "  'A man is playing guitar',\n",
    "  'I love pasta',\n",
    "  'The new movie is awesome',\n",
    "  'The cat plays in the garden',\n",
    "  'A woman watches TV',\n",
    "  'The new movie is so great',\n",
    "  'Do you like pizza?'\n",
    "]\n",
    "\n",
    "paraphrases = util.paraphrase_mining(model, sentences)\n",
    "sim = {'Source Sentence': [], 'Paraphrase': [], 'Similarity Score': []}\n",
    "for paraphrase in paraphrases[0:10]:\n",
    "    score, i, j = paraphrase\n",
    "    sim['Source Sentence'].append(sentences[i])\n",
    "    sim['Paraphrase'].append(sentences[j])\n",
    "    sim['Similarity Score'].append(score)\n",
    "    \n",
    "pd.DataFrame(sim)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval & Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a search query, a **retrieval system** retrieves a large list of potentially relevant hits for the query using a **bi-encoder**. Then a **re-ranker** based on a **cross-encoder** scores the relevancy of all candidates for the given search query.\n",
    "\n",
    "The dataset we use here is the smaller Simple English Wikipedia as document collection to provide answers to user questions and search queries can be downloaded [here](http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:30:32.363879Z",
     "start_time": "2022-07-07T00:30:27.540179Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "wikipedia_filepath = '../project_dir/simplewiki-2020-11-01.jsonl.gz'\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "  for line in fIn:\n",
    "    data = json.loads(line.strip())\n",
    "    #passages.extend(data['paragraphs'])\n",
    "    passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:31:34.113488Z",
     "start_time": "2022-07-07T00:30:32.365745Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm_notebook(passages):\n",
    "  tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "queries = [\n",
    "  'What is the best orchestra in the world?',\n",
    "  'Number countries Europe',\n",
    "  'When did the cold war end?',\n",
    "  'How long do cats live?',\n",
    "  'How many people live in Toronto?',\n",
    "  'Oldest US president',\n",
    "  'Coldest place earth',\n",
    "  'Elon Musk year birth',\n",
    "  'Paris eiffel tower',\n",
    "  'Which US president was killed?',\n",
    "  'When is Chinese New Year',  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:31:37.431287Z",
     "start_time": "2022-07-07T00:31:34.115399Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "  print(search(query))\n",
    "  print(\"\\n\\n======================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence embeddings compute an embedding vector for a piece of text. While thinking sentences is natural here, this method can also be used on shorter phrases and longer text containing multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T00:32:07.375580Z",
     "start_time": "2022-07-07T00:32:07.247143Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'This framework generates embeddings for each input sentence',\n",
    "  'Sentences are passed as a list of string.',\n",
    "  'The quick brown fox jumps over the lazy dog.'\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "  print(\"Sentence:\", sentence)\n",
    "  print(\"Embedding:\", embedding)\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-lingual Retrieval \n",
    "\n",
    "We show a use case of sentence transformer model in cross-lingual retrieval: cross-lingual semantic-similarity search engine\n",
    "\n",
    "This notebook has been adapted from [this paper](https://aclanthology.org/2020.acl-demos.12/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Transformer Model\n",
    "\n",
    "We load `xlm-mlm-100-1280` transformer which is language model trained for masked token prediction task on a dataset with 100 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('xlm-mlm-100-1280')\n",
    "\n",
    "def embed_text(input):\n",
    "  return model.encode(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Multilingual Semantic Similarity\n",
    "\n",
    "With sentence embeddings in hand, we can take their dot-product to measure their semantically similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of calculating semantic similarity between two multilingual sentences\n",
    "sentence1 = \"natural language understanding\" #en\n",
    "sentence2 = \"comprensión del lenguaje natural\" #es\n",
    "# encode sentences to get their embeddings\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "# compute similarity scores of two embeddings\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Sentence 1:\", sentence1)\n",
    "print(\"Sentence 2:\", sentence2)\n",
    "print(\"Similarity score:\", cosine_scores.item())\n",
    "\n",
    "\n",
    "# Retrieve top K most similar sentences from a multilingual corpus given a query sentence\n",
    "corpus = [\"I like Python because I can build AI applications\",\n",
    "          \"Me gusta Python porque puedo hacer análisis de datos\",\n",
    "          \"The cat sits on the ground\",\n",
    "         \"El gato camina por la acera.\"]\n",
    "# encode corpus to get corpus embeddings\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "sentence = \"I like Javascript because I can build web applications\"\n",
    "# encode sentence to get sentence embeddings\n",
    "sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "# top_k results to return\n",
    "top_k=2\n",
    "# compute similarity scores of the sentence with the corpus\n",
    "cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
    "# Sort the results in decreasing order and get the first top_k\n",
    "top_results = np.argpartition(-cos_scores.cpu(), range(top_k))[0:top_k]\n",
    "print(\"\\nSentence:\", sentence)\n",
    "print(\"Top\", top_k, \"most similar sentences in corpus:\")\n",
    "for idx in top_results[0:top_k]:\n",
    "    print(corpus[idx], \"(Score: %.4f)\" % (cos_scores[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Multilingual Semantic Similarity\n",
    "\n",
    "With sentence embeddings in hand, we can take their dot-product to visualize how similar sentences are between languages. A darker color indicates the embeddings are semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_similarity(embeddings_1, embeddings_2, labels_1, labels_2,\n",
    "                         plot_title,\n",
    "                         plot_width=1200, plot_height=600,\n",
    "                         xaxis_font_size='12pt', yaxis_font_size='12pt'):\n",
    "\n",
    "  assert len(embeddings_1) == len(labels_1)\n",
    "  assert len(embeddings_2) == len(labels_2)\n",
    "\n",
    "  # cosine based text similarity\n",
    "  sim = util.pytorch_cos_sim(embeddings_1, embeddings_2)\n",
    "\n",
    "  embeddings_1_col, embeddings_2_col, sim_col = [], [], []\n",
    "  for i in range(len(embeddings_1)):\n",
    "    for j in range(len(embeddings_2)):\n",
    "      embeddings_1_col.append(labels_1[i])\n",
    "      embeddings_2_col.append(labels_2[j])\n",
    "      sim_col.append(np.float(sim[i][j]))\n",
    "  df = pd.DataFrame(zip(embeddings_1_col, embeddings_2_col, sim_col),\n",
    "                    columns=['embeddings_1', 'embeddings_2', 'sim'])\n",
    "\n",
    "  mapper = bokeh.models.LinearColorMapper(\n",
    "      palette=[*reversed(bokeh.palettes.YlOrRd[9])], low=df.sim.min(),\n",
    "      high=df.sim.max())\n",
    "\n",
    "  p = bokeh.plotting.figure(title=plot_title, x_range=labels_1,\n",
    "                            x_axis_location=\"above\",\n",
    "                            y_range=[*reversed(labels_2)],\n",
    "                            plot_width=plot_width, plot_height=plot_height,\n",
    "                            tools=\"save\",toolbar_location='below', tooltips=[\n",
    "                                ('pair', '@embeddings_1 ||| @embeddings_2'),\n",
    "                                ('sim', '@sim')])\n",
    "  p.rect(x=\"embeddings_1\", y=\"embeddings_2\", width=1, height=1, source=df,\n",
    "         fill_color={'field': 'sim', 'transform': mapper}, line_color=None)\n",
    "\n",
    "  p.title.text_font_size = '12pt'\n",
    "  p.axis.axis_line_color = None\n",
    "  p.axis.major_tick_line_color = None\n",
    "  p.axis.major_label_standoff = 16\n",
    "  p.xaxis.major_label_text_font_size = xaxis_font_size\n",
    "  p.xaxis.major_label_orientation = 0.25 * np.pi\n",
    "  p.yaxis.major_label_text_font_size = yaxis_font_size\n",
    "  p.min_border_right = 300\n",
    "\n",
    "  bokeh.io.output_notebook()\n",
    "  bokeh.io.show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some texts of different lengths in different languages.\n",
    "english_sentences = ['dog', 'Puppies are nice.', 'I enjoy taking long walks along the beach with my dog.']\n",
    "spanish_sentences = ['perro', 'Los cachorros son agradables.', 'Disfruto de dar largos paseos por la playa con mi perro.']\n",
    "\n",
    "# Compute embeddings.\n",
    "en_result = embed_text(english_sentences)\n",
    "es_result = embed_text(spanish_sentences)\n",
    "\n",
    "# visualize semantic similarity\n",
    "visualize_similarity(en_result, es_result, english_sentences, spanish_sentences, 'English-Spanish Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple cross-lingual document retrieval engine\n",
    "\n",
    "#### Download Data to Index\n",
    "Download news sentences in multiples languages (English and Spanish) from the [News Commentary Corpus](http://opus.nlpl.eu/News-Commentary-v11.php) [[1]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.673.2874&rep=rep1&type=pdf).\n",
    "\n",
    "To speed up the demo, we limit to 1000 sentences per language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(corpus_metadata):\n",
    "  language_to_sentences = {}\n",
    "  language_to_news_path = {}\n",
    "  for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "    zip_path = tf.keras.utils.get_file(\n",
    "        fname=zip_file,\n",
    "        origin='http://opus.nlpl.eu/download.php?f=News-Commentary/v11/moses/' + zip_file,\n",
    "        extract=True)\n",
    "    news_path = os.path.join(os.path.dirname(zip_path), news_file)\n",
    "    language_to_sentences[language_code] = pd.read_csv(news_path, sep='\\t', header=None)[0][:1000]\n",
    "    language_to_news_path[language_code] = news_path\n",
    "\n",
    "    print('{:,} {} sentences'.format(len(language_to_sentences[language_code]), language_name))\n",
    "  return language_to_sentences, language_to_news_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_metadata = [\n",
    "    ('en', 'en-es.txt.zip', 'News-Commentary.en-es.en', 'English'),\n",
    "    ('es', 'en-es.txt.zip', 'News-Commentary.en-es.es', 'Spanish'),\n",
    "]\n",
    "language_to_sentences, language_to_news_path = download_files(corpus_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get document embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "language_to_embeddings = {}\n",
    "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "  print('\\nComputing {} embeddings'.format(language_name))\n",
    "  with tqdm(total=len(language_to_sentences[language_code])) as pbar:\n",
    "    df = pd.read_csv(language_to_news_path[language_code],\n",
    "                             sep='\\t',header=None).head(sample_size)\n",
    "    for i in range(len(df)):\n",
    "      language_to_embeddings.setdefault(language_code, []).append(embed_text(df[0][i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an index of semantic vectors\n",
    "\n",
    "Use the [SimpleNeighbors](https://pypi.org/project/simpleneighbors/) library---which is a wrapper for the [Annoy](https://github.com/spotify/annoy) library---to efficiently look up results from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_index_trees = 40\n",
    "language_name_to_index = {}\n",
    "embedding_dimensions = len(list(language_to_embeddings.values())[0][0])\n",
    "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "  print('\\nAdding {} embeddings to index'.format(language_name))\n",
    "  index = SimpleNeighbors(embedding_dimensions, metric='cosine')\n",
    "\n",
    "  for i in trange(len(language_to_sentences[language_code])):\n",
    "    index.add_one(language_to_sentences[language_code][i], language_to_embeddings[language_code][i])\n",
    "\n",
    "  print('Building {} index with {} trees...'.format(language_name, num_index_trees))\n",
    "  index.build(n=num_index_trees)\n",
    "  language_name_to_index[language_name] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_index_trees = 60\n",
    "print('Computing mixed-language index')\n",
    "combined_index = SimpleNeighbors(embedding_dimensions, metric='cosine')\n",
    "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "  print('Adding {} embeddings to mixed-language index'.format(language_name))\n",
    "  for i in trange(len(language_to_sentences[language_code])):\n",
    "    annotated_sentence = '({}) {}'.format(language_name, language_to_sentences[language_code][i])\n",
    "    combined_index.add_one(annotated_sentence, language_to_embeddings[language_code][i])\n",
    "\n",
    "print('Building mixed-language index with {} trees...'.format(num_index_trees))\n",
    "combined_index.build(n=num_index_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the document retrieval pipeline\n",
    "\n",
    "1.   Retrieve sentences from the corpus that are semantically similar to the given query.\n",
    "2.   Cross-lingual: Issue queries in a distinct language than the indexed corpus\n",
    "3.   Mixed-corpus: Issue queries in one language and retrieve similar documents from multiple languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-lingual\n",
    "\n",
    "Issue queries in a distinct language than the indexed corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Display sents with scores in DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = 'The stock market fell four points.'\n",
    "index_language = 'Spanish'  #@param [\"English\", \"Spanish\"]\n",
    "num_results = 10  #@param {type:\"slider\", min:0, max:100, step:10}\n",
    "\n",
    "query_embedding = embed_text(sample_query)\n",
    "search_results = language_name_to_index[index_language].nearest(query_embedding, n=num_results)\n",
    "\n",
    "print('{} sentences similar to: \"{}\"\\n'.format(index_language, sample_query))\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = 'Desde entonces, el índice ha trepado por encima de 10.000.'\n",
    "index_language = 'English'  #@param [\"English\", \"Spanish\"]\n",
    "num_results = 10  #@param {type:\"slider\", min:0, max:100, step:10}\n",
    "\n",
    "query_embedding = embed_text(sample_query)\n",
    "search_results = language_name_to_index[index_language].nearest(query_embedding, n=num_results)\n",
    "\n",
    "print('{} sentences similar to: \"{}\"\\n'.format(index_language, sample_query))\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mixed-corpus capabilities\n",
    "\n",
    "Issue a query in English and the results will come from the any of the indexed languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'The stock market fell four points.'  #@param {type:\"string\"}\n",
    "num_results = 10  #@param {type:\"slider\", min:0, max:100, step:10}\n",
    "\n",
    "query_embedding = embed_text(query)\n",
    "search_results = combined_index.nearest(query_embedding, n=num_results)\n",
    "\n",
    "print('Multilingual sentences similar to: \"{}\"\\n'.format(query))\n",
    "search_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqU6fryZc2CDjTChc1LC5b",
   "include_colab_link": true,
   "name": "SIGIR 2022 Efficient Transformers for IR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
