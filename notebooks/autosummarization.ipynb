{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers - Use Cases Using HuggingFace Transformers Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on chapter 6 **Summarization** of the book **Natural Language Processing with Tranformers** and can be found [here](https://nbviewer.org/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB4PkWm8-pfn"
   },
   "source": [
    "## Imports, Inits, & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:36:38.495569Z",
     "start_time": "2022-07-05T22:36:30.526913Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, pickle, sys, warnings, tqdm, time, torch\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:54:19.914104Z",
     "start_time": "2022-07-05T22:54:19.800056Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric, column_text='article', column_summary='abstract'):\n",
    "  summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "  metric.add_batch(predictions=summaries, references=dataset[column_summary])    \n",
    "  score = metric.compute()\n",
    "  return score\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "  \"\"\"\n",
    "  Yield successive batch-sized chunks from list_of_elements.\n",
    "  \"\"\"\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
    "                               batch_size=8, device=device, column_text='article',\n",
    "                               column_summary='abstract'):\n",
    "  article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "  target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "  for article_batch, target_batch in tqdm_notebook(zip(article_batches, target_batches), total=len(article_batches)):\n",
    "    inputs = tokenizer(article_batch, max_length=1024,  truncation=True, padding='max_length', return_tensors='pt')\n",
    "    summaries = model.generate(input_ids=inputs['input_ids'].to(device),\n",
    "                               attention_mask=inputs['attention_mask'].to(device),\n",
    "                               length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "    decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                          clean_up_tokenization_spaces=True)\n",
    "                         for s in summaries]\n",
    "    decoded_summaries = [d.replace('<n>', ' ') for d in decoded_summaries]\n",
    "    metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "  score = metric.compute()\n",
    "  return score\n",
    "\n",
    "def convert_examples_to_features(example_batch):\n",
    "  input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "\n",
    "  return {'input_ids': input_encodings['input_ids'],\n",
    "          'attention_mask': input_encodings['attention_mask'],\n",
    "          'labels': target_encodings['input_ids']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace's Summarization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using for this task is the Pubmed Summarization dataset which consists of 119,924 pairs of articles and their corresponding abstracts.\n",
    "\n",
    "This dataset can be found in the Hugging Face hub [here](https://huggingface.co/datasets/ccdv/pubmed-summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:36:51.056126Z",
     "start_time": "2022-07-05T22:36:50.205911Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# art_idx = 40767\n",
    "art_idx = 2\n",
    "# dataset = load_dataset('cnn_dailymail', version='3.0.0')\n",
    "dataset = load_dataset('ccdv/pubmed-summarization')\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n",
    "\n",
    "sample_text = dataset['train'][art_idx]\n",
    "print(f\"Article (excerpt of 500 characters, total length: {len(sample_text['article'])}):\")\n",
    "print(sample_text['article'][:500])\n",
    "print(f\"\\nSummary (length: {len(sample_text['abstract'])}):\")\n",
    "print(sample_text['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the articles' length to 2000 characters to have the same input to all the models and due to memory restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:36:57.138629Z",
     "start_time": "2022-07-05T22:36:57.050309Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = dataset['train'][art_idx]['article'][:2000]\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Summaries using Different Models Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:37:05.179483Z",
     "start_time": "2022-07-05T22:37:05.093088Z"
    }
   },
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "  return '\\n'.join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries['baseline'] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding `TL;DR:` at the end of the article prompts the GPT-2 model to generate a summary instead to generating free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:37:57.287537Z",
     "start_time": "2022-07-05T22:37:10.787679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('text-generation', model='gpt2-xl')\n",
    "gpt2_query = sample_text + '\\nTL;DR:\\n' \n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries['gpt2'] = '\\n'.join(sent_tokenize(pipe_out[0]['generated_text'][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 transformer is a universal trasnformer architecture by formulating all tasks as text-to-text tasks. T5 checkpoints are trained ona mixture of unsupervised data (to resconstruct masked words) and supervised data for several tasks including summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:40:37.779607Z",
     "start_time": "2022-07-05T22:40:09.702371Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='t5-large')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['t5'] = '\\n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART also uses an encoder-decoder architecture and is trained to reconstruct corrupted inputs. It combines pretraining schemes of BERT and GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T22:40:47.814652Z",
     "start_time": "2022-07-05T22:40:37.782915Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['bart'] = '\\n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEAGSUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEAGSUS is also an encoder-decoder architecture that is based on the premise that the closer the pretraining objective is to the downstream task, the more effectifve it is. In a very large corpus, sentences containing most of the content in their surrounding paragraphs can be reconstructed to obtain a SOTA model for text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T23:00:30.604714Z",
     "start_time": "2022-07-05T22:58:58.289401Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='google/pegasus-pubmed')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['pegasus'] = pipe_out[0]['summary_text'].replace(' .<n>', '.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Generated Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T23:00:31.232818Z",
     "start_time": "2022-07-05T23:00:30.608272Z"
    }
   },
   "outputs": [],
   "source": [
    "print('GROUND TRUTH')\n",
    "print(dataset['train'][art_idx]['abstract'])\n",
    "print('')\n",
    "\n",
    "for model_name in summaries:\n",
    "  print(model_name.upper())\n",
    "  print(summaries[model_name])\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating using ROGUE Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROUGE score was developed for applications like summarization where high recall is more important than precision. ROUGE is calculated based on how many `n`-grams in the reference text also occur in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T23:01:33.788105Z",
     "start_time": "2022-07-05T23:01:29.529877Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_metric = load_metric('rouge', chace_dir=None)\n",
    "rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "test_sampled = dataset['test'].shuffle(seed=42).select(range(250))\n",
    "\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "rogue_scores = pd.DataFrame.from_dict(rouge_dict, orient='index', columns=['baseline']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T23:04:53.356982Z",
     "start_time": "2022-07-05T23:01:45.435194Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"google/pegasus-pubmed\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=4)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "rogue_scores = rogue_scores.append(pd.DataFrame(rouge_dict, index=[\"pegasus\"]))\n",
    "rogue_scores"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqU6fryZc2CDjTChc1LC5b",
   "include_colab_link": true,
   "name": "SIGIR 2022 Efficient Transformers for IR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
