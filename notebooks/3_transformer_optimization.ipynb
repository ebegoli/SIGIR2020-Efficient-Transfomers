{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on chapter 8 **Making Transformers Efficient in Production** of the book **Natural Language Processing with Tranformers** and can be found [here](https://nbviewer.org/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB4PkWm8-pfn"
   },
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T19:53:39.134219Z",
     "start_time": "2022-07-07T19:53:33.330519Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, pickle, sys, warnings, tqdm, time, torch, json, gzip\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from benchmark import PerformanceBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T19:53:39.191885Z",
     "start_time": "2022-07-07T19:53:39.136268Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(ax,perf_metrics, current_optim_type):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        # Add a dashed circle around the current optimization type\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100, \n",
    "                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx, \n",
    "                        marker='$\\u25CC$')\n",
    "        else:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100, \n",
    "                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n",
    "            \n",
    "    legend = plt.legend(bbox_to_anchor=(1,1))\n",
    "    for handle in legend.legendHandles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylim(80,90)\n",
    "    # Use the slowest model to define the x-axis range\n",
    "    xlim = int(perf_metrics[\"BERT Baseline\"][\"time_avg_ms\"] + 3)\n",
    "    plt.xlim(1, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using is the `CLINC150` dataset for intent detection which can be found [here](https://huggingface.co/datasets/clinc_oos). This dataset includes 22,500 in-scope queries across 150 intents and 10 domains like banking and travel, and also includes 1,200 out-of-score queries that belong to `oos` intent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T19:53:39.846287Z",
     "start_time": "2022-07-07T19:53:39.193416Z"
    }
   },
   "outputs": [],
   "source": [
    "clinc = load_dataset('clinc_oos', 'plus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T19:55:21.872886Z",
     "start_time": "2022-07-07T19:53:39.848148Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "optim_type='BERT Baseline'\n",
    "pipe = pipeline('text-classification', model='transformersbook/bert-base-uncased-finetuned-clinc')\n",
    "pb = PerformanceBenchmark(pipe, clinc['test'], optim_type=optim_type)\n",
    "perf_metrics = pb.run_benchmark(clinc['test'].features['intent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Models Smaller via Knowledge Distillation - DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation is a method for training a *smaller* student model to mimic the behavior of a slower, larger, but better-performing *teacher*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T19:56:22.876525Z",
     "start_time": "2022-07-07T19:55:21.876407Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "optim_type='Distillation'\n",
    "pipe = pipeline('text-classification', model='transformersbook/distilbert-base-uncased-distilled-clinc')\n",
    "pb = PerformanceBenchmark(pipe, clinc['test'], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark(clinc['test'].features['intent']))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "plot_metrics(ax, perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Models Faster with Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization makes computations much more efficient by representing weights and activations with low-precision data types like 8-bit integer instead of 32-bit floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T19:57:54.498232Z",
     "start_time": "2022-07-07T19:56:22.878003Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_ckpt = 'transformersbook/distilbert-base-uncased-distilled-clinc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt).to(\"cpu\"))\n",
    "model_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "optim_type='Distillation + Quantization'\n",
    "pipe = pipeline('text-classification', model=model_quantized, tokenizer=tokenizer)\n",
    "pb = PerformanceBenchmark(pipe, clinc['test'], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark(clinc['test'].features['intent']))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "plot_metrics(ax, perf_metrics, optim_type)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqU6fryZc2CDjTChc1LC5b",
   "include_colab_link": true,
   "name": "SIGIR 2022 Efficient Transformers for IR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
