{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this notebook is based on chapter 6 **Summarization** of the book **Natural Language Processing with Tranformers** and can be found [here](https://nbviewer.org/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB4PkWm8-pfn"
   },
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:57:49.636915Z",
     "start_time": "2022-07-08T23:57:42.195790Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, pickle, sys, warnings, tqdm, time, torch\n",
    "warnings.filterwarnings(action='ignore')\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from termcolor import colored\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer,\n",
    "                          AutoModelForQuestionAnswering)\n",
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from transformers.data.processors.squad import SquadV1Processor\n",
    "from datasets import load_dataset, load_metric\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string, re\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:57:49.709093Z",
     "start_time": "2022-07-08T23:57:49.639512Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric, column_text='article', column_summary='abstract'):\n",
    "  summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "  metric.add_batch(predictions=summaries, references=dataset[column_summary])    \n",
    "  score = metric.compute()\n",
    "  return score\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "  \"\"\"\n",
    "  Yield successive batch-sized chunks from list_of_elements.\n",
    "  \"\"\"\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
    "                               batch_size=8, device=device, column_text='article',\n",
    "                               column_summary='abstract'):\n",
    "  article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "  target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "  for article_batch, target_batch in tqdm_notebook(zip(article_batches, target_batches), total=len(article_batches)):\n",
    "    inputs = tokenizer(article_batch, max_length=1024,  truncation=True, padding='max_length', return_tensors='pt')\n",
    "    summaries = model.generate(input_ids=inputs['input_ids'].to(device),\n",
    "                               attention_mask=inputs['attention_mask'].to(device),\n",
    "                               length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "    decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                          clean_up_tokenization_spaces=True)\n",
    "                         for s in summaries]\n",
    "    decoded_summaries = [d.replace('<n>', ' ') for d in decoded_summaries]\n",
    "    metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "  score = metric.compute()\n",
    "  return score\n",
    "\n",
    "def convert_examples_to_features(example_batch):\n",
    "  input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "\n",
    "  return {'input_ids': input_encodings['input_ids'],\n",
    "          'attention_mask': input_encodings['attention_mask'],\n",
    "          'labels': target_encodings['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_example(qid):    \n",
    "  idx = qid_to_example_index[qid]\n",
    "  q = examples[idx].question_text\n",
    "  c = examples[idx].context_text\n",
    "  a = [answer['text'] for answer in examples[idx].answers]\n",
    "\n",
    "  print(f'Example {idx} of {len(examples)}\\n---------------------')\n",
    "  print(f\"Q: {q}\\n\")\n",
    "  print(\"Context:\")\n",
    "  print(c)\n",
    "  print(f\"\\nTrue Answers:\\n{a}\")\n",
    "  \n",
    "def get_prediction(qid):\n",
    "  # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
    "  question = examples[qid_to_example_index[qid]].question_text\n",
    "  context = examples[qid_to_example_index[qid]].context_text\n",
    "\n",
    "  inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "  outputs = model(**inputs)\n",
    "  answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
    "  answer_end = torch.argmax(outputs[1]) + 1 \n",
    "\n",
    "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "  return answer\n",
    "  \n",
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "def normalize_text(s):\n",
    "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    return re.sub(regex, \" \", text)\n",
    "\n",
    "  def white_space_fix(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "  return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "  pred_tokens = normalize_text(prediction).split()\n",
    "  truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "    return int(pred_tokens == truth_tokens)\n",
    "\n",
    "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "  # if there are no common tokens then f1 = 0\n",
    "  if len(common_tokens) == 0:\n",
    "    return 0\n",
    "\n",
    "  prec = len(common_tokens) / len(pred_tokens)\n",
    "  rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "  return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "    \n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "    \n",
    "    return gold_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace's Summarization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using for this task is the Pubmed Summarization dataset which consists of 119,924 pairs of articles and their corresponding abstracts.\n",
    "\n",
    "This dataset can be found in the Hugging Face hub [here](https://huggingface.co/datasets/ccdv/pubmed-summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:57:50.737200Z",
     "start_time": "2022-07-08T23:57:49.710946Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "art_idx = 2\n",
    "dataset = load_dataset('ccdv/pubmed-summarization')\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n",
    "\n",
    "sample_text = dataset['train'][art_idx]\n",
    "print(f\"Article (excerpt of 500 characters, total length: {len(sample_text['article'])}):\")\n",
    "print(sample_text['article'][:500])\n",
    "print(f\"\\nSummary (length: {len(sample_text['abstract'])}):\")\n",
    "print(sample_text['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the articles' length to 2000 characters to have the same input to all the models and due to memory restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:57:50.827516Z",
     "start_time": "2022-07-08T23:57:50.741337Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = dataset['train'][art_idx]['article'][:2000]\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Summaries using Different Models Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primitive Summarization - Just take first 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:57:50.901827Z",
     "start_time": "2022-07-08T23:57:50.829036Z"
    }
   },
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "  return '\\n'.join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries['baseline'] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarization with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding `TL;DR:` at the end of the article prompts the GPT-2 model to generate a summary instead to generating free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:58:40.989814Z",
     "start_time": "2022-07-08T23:57:50.903144Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('text-generation', model='gpt2-xl')\n",
    "gpt2_query = sample_text + '\\nTL;DR:\\n' \n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries['gpt2'] = '\\n'.join(sent_tokenize(pipe_out[0]['generated_text'][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 transformer is a universal trasnformer architecture by formulating all tasks as text-to-text tasks. T5 checkpoints are trained ona mixture of unsupervised data (to resconstruct masked words) and supervised data for several tasks including summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:59:08.544072Z",
     "start_time": "2022-07-08T23:58:40.992549Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='t5-large')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['t5'] = '\\n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART also uses an encoder-decoder architecture and is trained to reconstruct corrupted inputs. It combines pretraining schemes of BERT and GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:59:18.783610Z",
     "start_time": "2022-07-08T23:59:08.547725Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['bart'] = '\\n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEAGSUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEAGSUS is also an encoder-decoder architecture that is based on the premise that the closer the pretraining objective is to the downstream task, the more effectifve it is. In a very large corpus, sentences containing most of the content in their surrounding paragraphs can be reconstructed to obtain a SOTA model for text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:59:52.379728Z",
     "start_time": "2022-07-08T23:59:18.785794Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='google/pegasus-pubmed')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['pegasus'] = pipe_out[0]['summary_text'].replace(' .<n>', '.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Generated Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T23:59:52.772325Z",
     "start_time": "2022-07-08T23:59:52.688801Z"
    }
   },
   "outputs": [],
   "source": [
    "print(colored('Original', 'red'))\n",
    "print(colored(dataset['train'][art_idx]['abstract'], 'green'))\n",
    "print('')\n",
    "\n",
    "for model_name in summaries:\n",
    "  print(colored(model_name.upper(), 'red'))\n",
    "  print(colored(summaries[model_name], 'blue'))\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating using ROGUE Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROUGE score was developed for applications like summarization where high recall is more important than precision. ROUGE is calculated based on how many `n`-grams in the reference text also occur in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T14:33:26.312972Z",
     "start_time": "2022-07-08T14:33:22.525173Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_metric = load_metric('rouge', chace_dir=None)\n",
    "rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "test_sampled = dataset['test'].shuffle(seed=42).select(range(250))\n",
    "\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "rogue_scores = pd.DataFrame.from_dict(rouge_dict, orient='index', columns=['baseline']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T14:33:26.312972Z",
     "start_time": "2022-07-08T14:33:22.525173Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"google/pegasus-pubmed\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=4)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "rogue_scores = rogue_scores.append(pd.DataFrame(rouge_dict, index=[\"pegasus\"]))\n",
    "rogue_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace's QA pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset using HuggingFace data processors\n",
    "\n",
    "The test dataset we are using for this task is the SQuAD-1.1 MRC dataset which consists of 10,570 question-answer pairs and 48 unique contexts.\n",
    "\n",
    "[Stanford Question Answering Dataset (SQuAD)](https://arxiv.org/abs/1606.05250) is the first large-scale extractive MRC dataset. It was built by crowd-workers using a set of Wikipedia articles where the answer to each question is an extracted slice from the corresponding text passage.\n",
    "\n",
    "This dataset can be downloaded from the Hugging Face hub [here](https://huggingface.co/datasets/squad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T16:34:03.504357Z",
     "start_time": "2022-07-08T16:33:59.464940Z"
    }
   },
   "outputs": [],
   "source": [
    "path_data = \"/net/kdinxidk03/opt/NFS/collab_dir/SIGIR2022/\"\n",
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(path_data, filename=\"squad/test_squad.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T16:34:05.521507Z",
     "start_time": "2022-07-08T16:34:05.401722Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('An example sample in the SQuAD-1.1 development set: ',\n",
    "      '\\n\\nQuestion: ', examples[10].__dict__['question_text'],\n",
    "      '\\n\\nContext: ', examples[10].__dict__['context_text'],\n",
    "      '\\n\\nAnswer: ', examples[10].__dict__['answers'][0]['text'],\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T16:34:09.761398Z",
     "start_time": "2022-07-08T16:34:09.639246Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qid_to_example_index = {example.qas_id: i for i, example in enumerate(examples)}\n",
    "answer_qids = list(qid_to_example_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a transformer model fine-tuned on SQuAD-1.1 for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T16:34:18.989563Z",
     "start_time": "2022-07-08T16:34:15.622907Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"csarron/bert-base-uncased-squad-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prediction and evaluate the prediction using EM & F1 metrics\n",
    "\n",
    "* Metrics for QA:\n",
    "\n",
    "1. Exact Match (EM): For each question+answer pair, if the _characters_ of the model's prediction exactly match the characters of (one of) the True Answer(s), EM = 1, otherwise EM = 0. This is a strict all-or-nothing metric; being off by a single character results in a score of 0.\n",
    "\n",
    "2. F1: F1 score is a common metric for classification problems, and widely used in QA. It is appropriate when we care equally about precision and recall. In this case, it's computed over the individual _words_ in the prediction against those in the True Answer. The number of shared words between the prediction and the truth is the basis of the F1 score: precision is the ratio of the number of shared words to the total number of words in the _prediction_, and recall is the ratio of the number of shared words to the total number of words in the _ground truth_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T16:34:25.477346Z",
     "start_time": "2022-07-08T16:34:25.242163Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 200\n",
    "prediction = get_prediction(answer_qids[idx])\n",
    "example = examples[qid_to_example_index[answer_qids[idx]]]\n",
    "gold_answers = get_gold_answers(example)\n",
    "\n",
    "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
    "\n",
    "print(f\"\\nQuestion: {example.question_text}\")\n",
    "print(f\"\\nContext: {example.context_text}\")\n",
    "print(f\"\\nPrediction: {prediction}\")\n",
    "print(f\"\\nTrue Answers: {gold_answers}\")\n",
    "# print(f\"\\nPerformance Scores: EM: {em_score} \\t F1: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqU6fryZc2CDjTChc1LC5b",
   "include_colab_link": true,
   "name": "SIGIR 2022 Efficient Transformers for IR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
