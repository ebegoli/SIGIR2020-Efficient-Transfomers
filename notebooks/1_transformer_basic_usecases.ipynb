{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers - Autosummarization Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on chapter 6 **Summarization** of the book **Natural Language Processing with Tranformers** and can be found [here](https://nbviewer.org/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB4PkWm8-pfn"
   },
   "source": [
    "## Imports, Inits, and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:44:49.990893Z",
     "start_time": "2022-07-06T18:44:46.228365Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, pickle, sys, warnings, tqdm, time, torch\n",
    "warnings.filterwarnings(action='ignore')\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:44:50.038798Z",
     "start_time": "2022-07-06T18:44:49.993434Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric, column_text='article', column_summary='abstract'):\n",
    "  summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "  metric.add_batch(predictions=summaries, references=dataset[column_summary])    \n",
    "  score = metric.compute()\n",
    "  return score\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "  \"\"\"\n",
    "  Yield successive batch-sized chunks from list_of_elements.\n",
    "  \"\"\"\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
    "                               batch_size=8, device=device, column_text='article',\n",
    "                               column_summary='abstract'):\n",
    "  article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "  target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "  for article_batch, target_batch in tqdm_notebook(zip(article_batches, target_batches), total=len(article_batches)):\n",
    "    inputs = tokenizer(article_batch, max_length=1024,  truncation=True, padding='max_length', return_tensors='pt')\n",
    "    summaries = model.generate(input_ids=inputs['input_ids'].to(device),\n",
    "                               attention_mask=inputs['attention_mask'].to(device),\n",
    "                               length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "    decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                          clean_up_tokenization_spaces=True)\n",
    "                         for s in summaries]\n",
    "    decoded_summaries = [d.replace('<n>', ' ') for d in decoded_summaries]\n",
    "    metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "  score = metric.compute()\n",
    "  return score\n",
    "\n",
    "def convert_examples_to_features(example_batch):\n",
    "  input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "\n",
    "  return {'input_ids': input_encodings['input_ids'],\n",
    "          'attention_mask': input_encodings['attention_mask'],\n",
    "          'labels': target_encodings['input_ids']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace's Summarization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using for this task is the Pubmed Summarization dataset which consists of 119,924 pairs of articles and their corresponding abstracts.\n",
    "\n",
    "This dataset can be found in the Hugging Face hub [here](https://huggingface.co/datasets/ccdv/pubmed-summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:44:50.926576Z",
     "start_time": "2022-07-06T18:44:50.040463Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: pubmed-summarization/section\n",
      "Reusing dataset pubmed-summarization (/net/kdinxidk03/opt/NFS/huggingface_cache/datasets/ccdv___pubmed-summarization/section/1.0.0/f9a2a592892a29ff6b5579891a7d5fcc3e15642540af26d1f6ac99086dc9fecd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c014f379d26940eda625aa5a082ff86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'abstract']\n",
      "Article (excerpt of 500 characters, total length: 7419):\n",
      "tardive dystonia ( td ) , a rarer side effect after longer exposure to antipsychotics , is characterized by local or general , sustained , involuntary contraction of a muscle or muscle group , with twisting movements , generally slow , which may affect the limbs , trunk , neck , or face . \n",
      " td has been shown to develop in about 3% of patients who have had long - term exposure to antipsychotics . \n",
      " . the low risk of td for atypical antipsychotics is thought to result from their weak affinity for \n",
      "\n",
      "Summary (length: 1009):\n",
      "tardive dystonia ( td ) is a serious side effect of antipsychotic medications , more with typical antipsychotics , that is potentially irreversible in affected patients . \n",
      " studies show that newer atypical antipsychotics have a lower risk of td . as a result , many clinicians may have developed a false sense of security when prescribing these medications . \n",
      " we report a case of 20-year - old male with hyperthymic temperament and borderline intellectual functioning , who developed severe td after low dose short duration exposure to atypical antipsychotic risperidone and then olanzapine . \n",
      " the goal of this paper is to alert the reader to be judicious and cautious before using casual low dose second generation antipsychotics in patient with no core psychotic features , hyperthymic temperament , or borderline intellectual functioning suggestive of organic brain damage , who are more prone to develop adverse effects such as td and monitor the onset of td in patients taking atypical antipsychotics .\n"
     ]
    }
   ],
   "source": [
    "# art_idx = 40767\n",
    "art_idx = 2\n",
    "# dataset = load_dataset('cnn_dailymail', version='3.0.0')\n",
    "dataset = load_dataset('ccdv/pubmed-summarization')\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n",
    "\n",
    "sample_text = dataset['train'][art_idx]\n",
    "print(f\"Article (excerpt of 500 characters, total length: {len(sample_text['article'])}):\")\n",
    "print(sample_text['article'][:500])\n",
    "print(f\"\\nSummary (length: {len(sample_text['abstract'])}):\")\n",
    "print(sample_text['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the articles' length to 2000 characters to have the same input to all the models and due to memory restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:44:50.974140Z",
     "start_time": "2022-07-06T18:44:50.929521Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = dataset['train'][art_idx]['article'][:2000]\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Summaries using Different Models Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:44:51.021466Z",
     "start_time": "2022-07-06T18:44:50.975607Z"
    }
   },
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "  return '\\n'.join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries['baseline'] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding `TL;DR:` at the end of the article prompts the GPT-2 model to generate a summary instead to generating free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:45:43.287720Z",
     "start_time": "2022-07-06T18:44:51.022704Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-generation', model='gpt2-xl')\n",
    "gpt2_query = sample_text + '\\nTL;DR:\\n' \n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries['gpt2'] = '\\n'.join(sent_tokenize(pipe_out[0]['generated_text'][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 transformer is a universal trasnformer architecture by formulating all tasks as text-to-text tasks. T5 checkpoints are trained ona mixture of unsupervised data (to resconstruct masked words) and supervised data for several tasks including summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:46:10.247756Z",
     "start_time": "2022-07-06T18:45:43.290117Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('summarization', model='t5-large')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['t5'] = '\\n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART also uses an encoder-decoder architecture and is trained to reconstruct corrupted inputs. It combines pretraining schemes of BERT and GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:46:20.427072Z",
     "start_time": "2022-07-06T18:46:10.250124Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['bart'] = '\\n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEAGSUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEAGSUS is also an encoder-decoder architecture that is based on the premise that the closer the pretraining objective is to the downstream task, the more effectifve it is. In a very large corpus, sentences containing most of the content in their surrounding paragraphs can be reconstructed to obtain a SOTA model for text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:46:55.313261Z",
     "start_time": "2022-07-06T18:46:20.429060Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model='google/pegasus-pubmed')\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries['pegasus'] = pipe_out[0]['summary_text'].replace(' .<n>', '.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Generated Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:50:07.475034Z",
     "start_time": "2022-07-06T18:50:07.002611Z"
    }
   },
   "outputs": [],
   "source": [
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:51:16.860916Z",
     "start_time": "2022-07-06T18:51:16.339629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mGROUND TRUTH\u001b[0m\n",
      "\u001b[32mtardive dystonia ( td ) is a serious side effect of antipsychotic medications , more with typical antipsychotics , that is potentially irreversible in affected patients . \n",
      " studies show that newer atypical antipsychotics have a lower risk of td . as a result , many clinicians may have developed a false sense of security when prescribing these medications . \n",
      " we report a case of 20-year - old male with hyperthymic temperament and borderline intellectual functioning , who developed severe td after low dose short duration exposure to atypical antipsychotic risperidone and then olanzapine . \n",
      " the goal of this paper is to alert the reader to be judicious and cautious before using casual low dose second generation antipsychotics in patient with no core psychotic features , hyperthymic temperament , or borderline intellectual functioning suggestive of organic brain damage , who are more prone to develop adverse effects such as td and monitor the onset of td in patients taking atypical antipsychotics .\u001b[0m\n",
      "\n",
      "\u001b[31mBASELINE\u001b[0m\n",
      "\u001b[34mtardive dystonia ( td ) , a rarer side effect after longer exposure to antipsychotics , is characterized by local or general , sustained , involuntary contraction of a muscle or muscle group , with twisting movements , generally slow , which may affect the limbs , trunk , neck , or face .\n",
      "td has been shown to develop in about 3% of patients who have had long - term exposure to antipsychotics . \n",
      " . the low risk of td for atypical antipsychotics is thought to result from their weak affinity for dopamine receptors .\n",
      "compared with typical , \n",
      " atypical antipsychotic agents have a greater affinity for serotonin 5-ht2a than dopamine d2 receptors , with a low propensity to induce td .\u001b[0m\n",
      "\n",
      "\u001b[31mGPT2\u001b[0m\n",
      "\u001b[34mAntipsychotics are associated with an increased risk of dystonic syndrome when used longer term, most common outcome of short exposure (e.g., 2 months) is extrapyramidal syndrome.\n",
      "A recent uncontrolled multicentric study suggests that olanzapine might be a better alternative than risperidone 2 mg olanzapine is less prone to cause extrap\u001b[0m\n",
      "\n",
      "\u001b[31mT5\u001b[0m\n",
      "\u001b[34mtardive dystonia (td) is a rarer side effect after longer exposure to antipsychotics .\n",
      "it is characterized by local or general , sustained , involuntary contraction of a muscle or muscle group .\n",
      "may affect the limbs , trunk , neck , or face .\n",
      "td has been shown to develop in about 3% of patients who have had long - term exposure .\u001b[0m\n",
      "\n",
      "\u001b[31mBART\u001b[0m\n",
      "\u001b[34m tardive dystonia (td) is a rarer side effect after longer exposure to antipsychotics.\n",
      "It is characterized by local or general, sustained, involuntary contraction of a muscle or muscle group.\n",
      "It has been shown to develop in about 3% of patients who have had long - term exposure to  antipsychotic drugs.\u001b[0m\n",
      "\n",
      "\u001b[31mPEGASUS\u001b[0m\n",
      "\u001b[34mtardive dystonia ( td ) , a rarer side effect after longer exposure to antipsychotics , is characterized by local or general , sustained , involuntary contraction of a muscle or muscle group , with twisting movements , generally slow , which may affect the limbs , trunk , neck , or face . td has been shown to develop in about 3% of patients who have had long - term exposure to antipsychotics . <n> the low risk of td for atypical antipsychotics is thought to result from their weak affinity for dopamine receptors . compared with typical , atypical antipsychotic agents have a greater affinity for serotonin 5-ht2a than dopamine d2 receptors , with a low propensity to induce td . among this olanzapine is thought to have preferential action at mesolimbic over nigrostriatal dopaminergic pathways and is , therefore , associated with a very low incidence of extrapyramidal symptom ( eps ) . <n> furthermore , a retrospective analysis of controlled multicentric trials suggested that olanzapine also improves preexisting symptoms of tardive movements .\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(colored('GROUND TRUTH', 'red'))\n",
    "print(colored(dataset['train'][art_idx]['abstract'], 'green'))\n",
    "print('')\n",
    "\n",
    "for model_name in summaries:\n",
    "  print(colored(model_name.upper(), 'red'))\n",
    "  print(colored(summaries[model_name], 'blue'))\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating using ROGUE Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROUGE score was developed for applications like summarization where high recall is more important than precision. ROUGE is calculated based on how many `n`-grams in the reference text also occur in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:46:59.481822Z",
     "start_time": "2022-07-06T18:46:55.720206Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /net/kdinxidk03/opt/NFS/huggingface_cache/datasets/ccdv___pubmed-summarization/section/1.0.0/f9a2a592892a29ff6b5579891a7d5fcc3e15642540af26d1f6ac99086dc9fecd/cache-6df034ac805cbe09.arrow\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = load_metric('rouge', chace_dir=None)\n",
    "rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "test_sampled = dataset['test'].shuffle(seed=42).select(range(250))\n",
    "\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "rogue_scores = pd.DataFrame.from_dict(rouge_dict, orient='index', columns=['baseline']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:50:06.999826Z",
     "start_time": "2022-07-06T18:46:59.483489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd278a6e2e54474185a1412139615340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.270288</td>\n",
       "      <td>0.090744</td>\n",
       "      <td>0.168766</td>\n",
       "      <td>0.244628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.350126</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>0.226754</td>\n",
       "      <td>0.296896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.270288  0.090744  0.168766   0.244628\n",
       "pegasus   0.350126  0.154301  0.226754   0.296896"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-pubmed\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=4)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "rogue_scores = rogue_scores.append(pd.DataFrame(rouge_dict, index=[\"pegasus\"]))\n",
    "rogue_scores"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqU6fryZc2CDjTChc1LC5b",
   "include_colab_link": true,
   "name": "SIGIR 2022 Efficient Transformers for IR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
